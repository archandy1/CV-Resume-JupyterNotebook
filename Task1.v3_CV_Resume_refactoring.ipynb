{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9cfbe7-4b5a-46c9-a464-414908206d87",
   "metadata": {},
   "source": [
    "# 1.4.1. Task 1: Working with the documents \r\n",
    "\r\n",
    "## 1. Translation.\r\n",
    "Find the resumes (or parts of them) that are not in English, and translate them into English using LLM.\r\n",
    "\r\n",
    "## 2. Entities extraction.\r\n",
    "Extract useful named entities from the resume using LLM. For example, you can extract the job title, years of experience, highest level of education, language skills, and key skills, or define any entities that you find interesting. As an additional task, you may create an Excel report that contains entities from 20-30 resumes.\r\n",
    "\r\n",
    "## 3. Summarisation.\r\n",
    "Make a short summary of the resume. You may choose any size you find useful. Defining the structure of the summary (adding the obligatory entities) or just getting it from LLM is up to you. The general idea is to provide an opportunity for recruiters to read it quickly and not scan 2-3 pages.\r\n",
    "\r\n",
    "## 4. Resume scoring.\r\n",
    "Develop a mechanism to provide a ranking of the resumes for a vacancy by providing a score (float value from 0 to 1). A particular vacancy can be found at [https://www.dataart.team/vacancies](https://www.dataart.team/vacancies) (or on LinkedIn). It should work in 2 modes: calculate the score for the provided vacancy and resume, and present the top 10 candidates for the vacancy.\r\n",
    "a and algorithms for scoring.\r\n",
    "ncy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb6f51-6804-477d-8426-af1da337c17d",
   "metadata": {},
   "source": [
    "### Installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b50aa-449d-41e6-903b-eb6046b93528",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai pandas langdetect PyPDF2 faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f993a-2209-4021-a60f-2358e5a8a0b3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from langdetect import detect, DetectorFactory\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d88852-7c55-4dc6-a300-eb9d1947590e",
   "metadata": {},
   "source": [
    "### Set environment variable for Open Ai client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53202e6b-24ea-418a-80e4-1b227e6cbc32",
   "metadata": {},
   "source": [
    "### Create basic Variables, paths and set Open AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "directory_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/test_resumes_dataset\")\n",
    "translated_output_directory = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/resumes_translated\")\n",
    "logs_directory = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs\")\n",
    "max_chunk_size = 3500  \n",
    "overlap_size = 50  \n",
    "\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaf1e0-fd25-4546-94cf-bd631dfe6169",
   "metadata": {},
   "source": [
    "### extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a1ca4-65ff-45f3-95ec-7de30135ed81",
   "metadata": {},
   "source": [
    "### Split text into chunks with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_chunk_size, overlap_size=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) + 1 <= max_chunk_size:\n",
    "            current_chunk += word + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = word + \" \"\n",
    "    chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578372d7-9f7a-41fc-9578-b639c23ea7b7",
   "metadata": {},
   "source": [
    "### Function to translate text using OpenAI's API  for openai>=1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(client, text, target_language=\"en\"):\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"Translate the following text to {target_language}:\\n\\n{text}\",\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bf752-6889-40dd-85f1-59fbf8e0ce94",
   "metadata": {},
   "source": [
    "### Function to process PDFs and save translated versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(directory_path, max_chunk_size, overlap_size, translated_output_directory, client):\n",
    "    directory_path = Path(directory_path)\n",
    "    translated_output_directory = Path(translated_output_directory)\n",
    "    \n",
    "    translated_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    translated_files_list = []\n",
    "    english_files_list = []\n",
    "\n",
    "    for pdf_path in directory_path.glob('*.pdf'):  \n",
    "        text = extract_text_from_pdf(str(pdf_path))\n",
    "\n",
    "        if text.strip():\n",
    "            if detect(text) != 'en':\n",
    "                chunks = split_text(text, max_chunk_size, overlap_size)\n",
    "                translated_text = \"\"\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    if detect(chunk) != 'en':\n",
    "                        chunk = translate_text(client, chunk, target_language=\"en\")\n",
    "                    translated_text += chunk + \" \"\n",
    "                \n",
    "                translated_filename = f\"translated_{pdf_path.stem}.txt\"\n",
    "                translated_path = translated_output_directory / translated_filename\n",
    "                save_text_to_file(translated_text, translated_path)\n",
    "\n",
    "                translated_files_list.append(translated_filename)\n",
    "            else:\n",
    "                english_files_list.append(pdf_path.name)\n",
    "        else:\n",
    "            print(f\"Document {pdf_path.name} is empty or contains very little text.\")\n",
    "\n",
    "\n",
    "    save_file_list(translated_files_list, Path(logs_directory), 'translated_files_list.txt')\n",
    "    save_file_list(english_files_list, Path(logs_directory), 'english_files_list.txt')\n",
    "\n",
    "def save_file_list(file_list, directory, filename):\n",
    "    directory = Path(directory)  \n",
    "    file_path = directory / filename  \n",
    "    with file_path.open('w', encoding='utf-8') as f:\n",
    "        for file in file_list:\n",
    "            f.write(f\"{file}\\n\")\n",
    "\n",
    "\n",
    "def save_text_to_file(text, file_path):\n",
    "    file_path = Path(file_path)  \n",
    "    with file_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d3a14-a3dd-45df-b313-8565201961d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pdfs(directory_path, max_chunk_size, overlap_size, translated_output_directory, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4e8ef-b432-4801-80a7-db3b4c6ae79b",
   "metadata": {},
   "source": [
    "### Create named entities to look for in resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ce8ea-f58f-444c-b133-d7ec08c5066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\"job title\", \"years of experience\", \"highest level of education\", \"language skills\", \"key skills\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e41f2-9ec3-4c12-b528-596fdd864f71",
   "metadata": {},
   "source": [
    "### Function to extract text from TXT files with different encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f4e07-235c-4ea1-856f-4392afb44c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(file_path):\n",
    "    file_path = Path(file_path) \n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with file_path.open('r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(f\"Cannot decode file {file_path} with any of the provided encodings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd143b-5d23-4654-94f9-d0a7cb0d7706",
   "metadata": {},
   "source": [
    "### Function to extract entities using the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcae4f-d4f1-4962-9fe5-c5484229e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_with_llm(client, text, entities, max_chunk_size, overlap_size):\n",
    "    extracted_info = \"\"\n",
    "    chunks = split_text(text, max_chunk_size, overlap_size) \n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "                \"Extract the following entities from this text, calculating years of experience as a decimal number where months are converted to a fractional year without any additional info: \"\n",
    "                + \", \".join(entities)\n",
    "                + \". If a value for an entity is not present or cannot be extracted, fill in with just: NaN.\"\n",
    "                + \"\\n\\n\"\n",
    "                + chunk\n",
    "            )\n",
    "        prompt_length = len(prompt.split())\n",
    "\n",
    "        max_tokens_for_completion = 4097 - prompt_length\n",
    "        max_tokens_for_completion = min(max_tokens_for_completion, 300)\n",
    "\n",
    "        response = client.completions.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens_for_completion,\n",
    "            temperature=0.35\n",
    "        )\n",
    "        extracted_info += response.choices[0].text.strip() + \"\\n\"\n",
    "\n",
    "    return extracted_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36991005-0595-4572-9f5d-5d20daba21c0",
   "metadata": {},
   "source": [
    "### Function to process resumes and extract named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825042b0-1888-4be1-a8e0-795274758661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(directory, filename, client, entities, data_list, is_txt=False):\n",
    "    file_path = directory / filename \n",
    "    text = extract_text_from_txt(file_path) if is_txt else extract_text_from_pdf(file_path)\n",
    "\n",
    "    if text.strip():\n",
    "        extracted_info = extract_entities_with_llm(client, text, entities, max_chunk_size, overlap_size)\n",
    "        info_dict = {'Filename': filename}\n",
    "\n",
    "        for entity in entities:\n",
    "            pattern = re.compile(rf\"{entity}\\s*:\\s*(.*)\", re.IGNORECASE)\n",
    "            match = pattern.search(extracted_info)\n",
    "            if match:\n",
    "                info_dict[entity] = match.group(1).strip()\n",
    "            else:\n",
    "                info_dict[entity] = None\n",
    "\n",
    "        data_list.append(info_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0429b-fcf9-43ac-b053-98ea3c74e6b2",
   "metadata": {},
   "source": [
    "### Function to create a report of named entities from resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162738f-0d91-4bd0-ba4f-c05d60b29733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entities_report(directory_path, translated_output_directory, client, entities):\n",
    "    data = []\n",
    "    directory_path = Path(directory_path)\n",
    "    translated_output_directory = Path(translated_output_directory)\n",
    "\n",
    "    for pdf_path in directory_path.glob('*.pdf'):\n",
    "        process_resume(directory_path, pdf_path.name, client, entities, data)\n",
    "\n",
    "    for txt_path in translated_output_directory.glob('0_translated_*.txt'):\n",
    "        process_resume(translated_output_directory, txt_path.name, client, entities, data, is_txt=True)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    report_path = logs_directory / '1_resumes_entities_report.xlsx'\n",
    "    df.to_excel(report_path, index=False)\n",
    "\n",
    "create_entities_report(directory_path, translated_output_directory, client, entities)\n",
    "\n",
    "file_path = logs_directory / '1_resumes_entities_report.xlsx'\n",
    "\n",
    "if file_path.is_file():\n",
    "    df = pd.read_excel(file_path)\n",
    "    df.set_index('Filename', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    years_of_experience = df['years of experience']\n",
    "else:\n",
    "    print(f\"Error: The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919cbeb-4d82-49d3-a4d5-2ce1e51b85db",
   "metadata": {},
   "source": [
    "### Load the DataFrame from the Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cae43-9660-42a3-8a71-8fc4feef2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(logs_directory, '1_resumes_entities_report.xlsx')\n",
    "df = pd.read_excel(file_path)\n",
    "df.set_index('Filename', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "years_of_experience = df['years of experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28de2e-355c-4cbc-9a3c-ff7e20098da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec9855-0a73-4c4f-8ef7-eb6c86b98342",
   "metadata": {},
   "source": [
    "### Function to calculate numeric years of experience from text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9e913-5c27-4146-9c56-99ffa5d2a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/1_resumes_entities_report.xlsx\")\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "text_descriptions = df['years of experience'].astype(str).tolist()\n",
    "\n",
    "new_file_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/2_resumes_updated_years.xlsx\")\n",
    "\n",
    "def calculate_years_of_experience(client, text_descriptions):\n",
    "    numeric_experience_list = []\n",
    "\n",
    "    for text in text_descriptions:\n",
    "        prompt = f\"Convert the following description of work experience '{text}' into a numeric value representing total years of experience.\"\n",
    "\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=50,\n",
    "                temperature=0.1\n",
    "            )\n",
    "\n",
    "            extracted_numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', response.choices[0].text.strip())\n",
    "            if extracted_numbers:\n",
    "                numeric_experience = float(extracted_numbers[0])\n",
    "                numeric_experience_list.append(numeric_experience)\n",
    "            else:\n",
    "                numeric_experience_list.append(float('nan'))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            numeric_experience_list.append(float('nan'))\n",
    "\n",
    "    return numeric_experience_list\n",
    "\n",
    "numeric_years_of_experience = calculate_years_of_experience(client, text_descriptions)\n",
    "\n",
    "df['numeric_years_of_experience'] = numeric_years_of_experience\n",
    "\n",
    "df.to_excel(new_file_path, index=False)\n",
    "\n",
    "print(f\"The updated DataFrame has been saved to {new_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32378-9b06-4c95-9ee2-281b1f192332",
   "metadata": {},
   "source": [
    "### Function to summarize resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(client, text, max_chunk_size=3000, overlap_size=50):\n",
    "    chunks = split_text(text, max_chunk_size, overlap_size)\n",
    "    summary = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Please summarize the following resume into a short paragraph that includes \"\n",
    "            \"the job title, years of experience, highest level of education, language skills, \"\n",
    "            \"and key skills:\\n\\n\" + chunk\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=150,  \n",
    "                temperature=0.2\n",
    "            )\n",
    "            chunk_summary = response.choices[0].text.strip()\n",
    "            summary += chunk_summary + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e732f0-db46-41aa-a2d2-92a13692d183",
   "metadata": {},
   "source": [
    "### Process and Summarize Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727019f-b667-445f-89b7-12cb2b5739f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in df.index:\n",
    "    if filename.startswith('translated_'):\n",
    "        resume_path = translated_output_directory / filename\n",
    "    else:\n",
    "        resume_path = directory_path / filename\n",
    "\n",
    "    if 'translated_files_list' in filename:\n",
    "        continue\n",
    "\n",
    "    if resume_path.suffix.lower() == '.pdf':\n",
    "        try:\n",
    "            resume_text = extract_text_from_pdf(str(resume_path))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading PDF file: {e}\")\n",
    "            continue\n",
    "    elif resume_path.suffix.lower() == '.txt':\n",
    "        try:\n",
    "            resume_text = extract_text_from_txt(str(resume_path))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading text file: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Unsupported file format for file: {resume_path}\")\n",
    "        continue\n",
    "\n",
    "    summary = summarize_text(client, resume_text)\n",
    "    df.at[filename, 'Summary'] = summary\n",
    "\n",
    "print(df.head())\n",
    "updated_file_path = logs_directory / '3_resumes_summaries.xlsx'\n",
    "df.to_excel(str(updated_file_path))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe489854-05af-4012-ab60-43aa42ca201e",
   "metadata": {},
   "source": [
    "### Scoring criteria based on provided vacancy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d4721-b9da-4130-a26b-54597cc23b95",
   "metadata": {},
   "source": [
    "### job description from vacancies.DataArt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c70af-8248-4c50-9f36-0a2b24928eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "FullStack(NodeJS, ReactJS), Online Genealogy Service\n",
    "Client\n",
    "The client is an international company that provides an online genealogy service that helps its clients understand their past and family history.\n",
    "\n",
    "Project overview\n",
    "The core programming language is JavaScript (ES2020), a website running on React.js and GraphQL and the back-end platform is based on Node.js (Express). Microservices running under Kubernetes. The project methodology is Scrum.\n",
    "\n",
    "Team\n",
    "There are a few Full Stack teams, up to 8 people each. Each team has a team lead and a product owner.\n",
    "\n",
    "Position overview\n",
    "We are looking for a specialist to join one of the teams (which is more Frontend oriented) is working on the further development of existing platforms. Regarding the work schedule, each employee should be available till 4 pm UK time.\n",
    "\n",
    "Technology stack\n",
    "JavaScript, React.js, GraphQL, Node.js (Express), Kubernetes.\n",
    " \n",
    "Requirements\n",
    "Development experience using a Node.js (Express) + React.js stack\n",
    "Experience with SQL Server\n",
    "Experience with PostgreSQL\n",
    "Knowledge of Kafka\n",
    "Knowledge of RabbitMQ\n",
    "Dev-level experience with K8s/Docker\n",
    "Knowledge of sound engineering practices like pair programming, upfront automated testing, continuous deployment, and trunk-based development\n",
    "Spoken English\n",
    "\n",
    "Nice to have\n",
    "Knowledge of Apollo engine, Kafka, Postgres\n",
    "Experience with microservices architecture development\n",
    "Experience with GraphQL\n",
    "Experience with RabbitMQ, SQL Server\n",
    "Experience in development with C#\n",
    "Experience with SOLR\n",
    "Software development experience in Python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c81914-dcb6-46c6-b92a-4f7b8d3a23cb",
   "metadata": {},
   "source": [
    "### combine columns in excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0534b-d224-42c4-9faa-083597a851fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_1 = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/3_resumes_summaries.xlsx\")\n",
    "excel_file_2 = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/2_resumes_updated_years.xlsx\")\n",
    "\n",
    "df1 = pd.read_excel(excel_file_1)\n",
    "df2 = pd.read_excel(excel_file_2)\n",
    "\n",
    "df1.set_index('Filename', inplace=True)\n",
    "df2.set_index('Filename', inplace=True)\n",
    "\n",
    "df1['years of experience'] = df2['numeric_years_of_experience']\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "save_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/4_resumes_years_summaries.xlsx\")\n",
    "\n",
    "df1.to_excel(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994529b8-7b41-423f-9a76-9f3a9b429f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc891e45-cd88-4241-a314-57302b328ba0",
   "metadata": {},
   "source": [
    "### working embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257841b-9833-445b-9fa7-af5c73ab9164",
   "metadata": {},
   "source": [
    "### Read file as xlsx and save as CSV, clean excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544d8f4-d498-47db-9327-7e32f6dfb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def list_to_json_str(lst):\n",
    "    return json.dumps(lst)\n",
    "\n",
    "excel_file_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/4_resumes_years_summaries.xlsx\")\n",
    "\n",
    "df = pd.read_excel(excel_file_path, index_col='Filename')\n",
    "\n",
    "text_columns = ['job title', 'years of experience', 'highest level of education', 'language skills', 'key skills', 'Summary']\n",
    "\n",
    "for column in text_columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column + ' embedding'] = df[column].apply(lambda x: list_to_json_str(get_embedding(x)) if pd.notnull(x) else np.nan)\n",
    "\n",
    "save_csv_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/5_resumes_embeddings.csv\")\n",
    "df.to_csv(save_csv_path, index=True)\n",
    "\n",
    "save_excel_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/5_resumes_embeddings.xlsx\")\n",
    "df.to_excel(save_excel_path, index=True)\n",
    "\n",
    "print(f\"DataFrame saved to {save_csv_path} and {save_excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bcc8b-7ad3-49b0-8da9-a1bdd7f9f099",
   "metadata": {},
   "source": [
    "### Check if everything worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd5b77-40f1-4afa-9e4b-d3acd6d90ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/5_resumes_embeddings.csv\")\n",
    "df.head()\n",
    "df.tail()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83127fc-fc4d-4442-8a8c-4b70f3a64237",
   "metadata": {},
   "source": [
    "### Scoring using vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41c12e-a9c1-46ac-8f54-b1b7e9245b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from ast import literal_eval\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def string_to_float_list(s):\n",
    "    try:\n",
    "        return np.array(literal_eval(s))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "job_description = {\n",
    "    \"job title\": \"FullStack Developer\",\n",
    "    \"years of experience\": \"At least 2 years of development experience\",\n",
    "    \"highest level of education\": \"Bachelor's or higher in Computer Science or related field\",\n",
    "    \"language skills\": \"Fluent in spoken English\",\n",
    "    \"key skills\": \"Node.js, React.js, GraphQL, Kubernetes, SQL Server, PostgreSQL, Kafka, RabbitMQ, C#, SOLR, Python, Sound engineering practices, Pair programming, Automated testing, Continuous deployment, Trunk-based development\"\n",
    "}\n",
    "\n",
    "job_description_embeddings = {key: get_embedding(value) for key, value in job_description.items()}\n",
    "\n",
    "df_path = \"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/5_resumes_embeddings.csv\"\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "embedding_columns = [col for col in df.columns if 'embedding' in col]\n",
    "df[embedding_columns] = df[embedding_columns].applymap(string_to_float_list)\n",
    "\n",
    "def search_resumes(df, job_description_embeddings):\n",
    "    df['similarity'] = 0.0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        similarity_scores = []\n",
    "        for key, job_embedding in job_description_embeddings.items():\n",
    "            embedding_col_name = f'{key} embedding'\n",
    "\n",
    "            if embedding_col_name in df.columns:\n",
    "                resume_embedding = row.get(embedding_col_name)\n",
    "\n",
    "                if isinstance(resume_embedding, np.ndarray) and not np.isnan(resume_embedding).any():\n",
    "                    similarity_scores.append(cosine_similarity(resume_embedding, job_embedding))\n",
    "\n",
    "        if similarity_scores:\n",
    "            df.at[index, 'similarity'] = np.mean(similarity_scores)\n",
    "        else:\n",
    "            df.at[index, 'similarity'] = 0.0  \n",
    "\n",
    "    sorted_df = df.sort_values('similarity', ascending=False)\n",
    "\n",
    "    csv_file_path = \"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/6_scores.csv\"\n",
    "    sorted_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"DataFrame saved to {csv_file_path}\")\n",
    "\n",
    "    return sorted_df\n",
    "\n",
    "top_matches = search_resumes(df, job_description_embeddings)\n",
    "top_matches.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8171b5e-2874-4786-9cdc-6d283569277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7749e-d3d6-420e-ae76-ea283c15de3a",
   "metadata": {},
   "source": [
    "### Combine scores with existing excel file and save under new name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161bf32-151e-4ec2-9f72-a163a39ce952",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/4_resumes_years_summaries.xlsx\")\n",
    "csv_file_path = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/6_scores.csv\")\n",
    "output_excel_file = Path(\"C:/Users/apleczkan/PycharmProjects/task1-cv-resumes/logs/7_resumes_summary_scores_sorted.xlsx\")\n",
    "\n",
    "excel_df = pd.read_excel(excel_file)\n",
    "scores_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "excel_df = excel_df.sort_values(by='Filename')\n",
    "scores_df = scores_df.sort_values(by='Filename')\n",
    "\n",
    "scores_df = scores_df.rename(columns={'similarity': 'scores'})\n",
    "\n",
    "excel_df['scores'] = scores_df['scores'].values\n",
    "\n",
    "if \"Unnamed: 0\" in excel_df.columns:\n",
    "    excel_df = excel_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "sorted_df = excel_df.sort_values(by='scores', ascending=False)\n",
    "\n",
    "sorted_df.to_excel(output_excel_file, index=False)\n",
    "\n",
    "print(f\"Sorted and saved DataFrame to {output_excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d7584-260e-410d-ae11-ad6d3d5f12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sorted DataFrame:\")\n",
    "sorted_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
